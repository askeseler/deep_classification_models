{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "fIxslsTL_31x"
   },
   "outputs": [],
   "source": [
    "#Import suppporting libraries\n",
    "import tarfile\n",
    "import urllib.request as urllib2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "#Import deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.models as models\n",
    "#Import data analytics libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#Import image visualization libraries\n",
    "from PIL import *\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "#System settings\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "#Coloring for print outputs\n",
    "class color:\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfjytRH6w2vO"
   },
   "source": [
    "When dealing with image classification, people often start by classifying one or more categories within a class. For example, if you want to classify cars, you could make the distinction of whether it is a convertible or not. This would be an example of binary classification. A more complex task could be to distinguish between several categories. Is it an Audi, a BMW, a Mercedes or a Ford? There is more than one category within the car brand. What if we want to combine both examples? We could classify multiple features at once for each image showing a vehicle, e.g. the brand, the vehicle type, and the year of manufacture. One way would be to train three independent classifiers, but it is also possible to integrate everything into one model. We will do this together with the Stanford Car Dataset. Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7P6SEpzAB_n",
    "outputId": "e0bba753-1e33-4ed9-a164-f9da919383f4"
   },
   "outputs": [],
   "source": [
    "def getting_data(url,path):\n",
    "  data = urllib2.urlopen(url)\n",
    "  tar_package = tarfile.open(fileobj=data, mode='r:gz')\n",
    "  tar_package.extractall(path)\n",
    "  tar_package.close()\n",
    "  return print(\"Data extracted and saved.\")\n",
    "\n",
    "#getting_data(\"http://imagenet.stanford.edu/internal/car196/car_ims.tgz\",\"/content/carimages\")\n",
    "\n",
    "def getting_metadata(url,filename):\n",
    "  '''\n",
    "  Downloading a metadata file from a specific url and save it to the disc.\n",
    "  '''\n",
    "  labels = urllib2.urlopen(url)\n",
    "  file = open(filename, 'wb')\n",
    "  file.write(labels.read())\n",
    "  file.close()\n",
    "  return print(\"Metadata downloaded and saved.\")\n",
    "\n",
    "#getting_metadata(\"http://ai.stanford.edu/~jkrause/car196/cars_annos.mat\",\"car_metadata.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzRCB2tdy3Cv"
   },
   "source": [
    "First, we created two functions to a) download and extract the images themselves and b) store the corresponding metadata (containing information about the brand and model). In a next step, we create a class that merges both information and extracts a total of three relevant features:\n",
    "1. All brands in the dataset with more than 1000 images. We put all other brands into the category \"Other\".\n",
    "2. We distinguish between different types of vehicles: Convertible, Coupe, SUV, Van.  All models without reference to the vehicle type, we summarize to the category \"Other\".\n",
    "3. We divide the carpool into two cohorts: All cars released in 2009 and earlier and all cars released in 2010 and later.\n",
    "\n",
    "So we have three targets with different classes, each of which we want to predict all at the same time. We can extract all the needed information from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "H4XeKTcn7BKE"
   },
   "outputs": [],
   "source": [
    "class MetaParsing():\n",
    "  '''\n",
    "  Class for parsing image and meta-data for the Stanford car dataset to create a custom dataset.\n",
    "  path: The filepah to the metadata in .mat format.\n",
    "  *args: Accepts dictionaries with self-created labels which will be extracted from the metadata (e.g. {0: 'Audi', 1: 'BMW', 3: 'Other').\n",
    "  year: Can be defined to create two classes (<=year and later).\n",
    "  '''\n",
    "  def __init__(self,path,*args,year=None):\n",
    "    self.mat = scipy.io.loadmat(path)\n",
    "    self.year = year\n",
    "    self.args = args\n",
    "    self.annotations = np.transpose(self.mat['annotations'])\n",
    "    #Extracting the file name for each sample\n",
    "    self.file_names = [annotation[0][0][0].split(\"/\")[-1] for annotation in self.annotations]\n",
    "    #Extracting the index of the label for each sample\n",
    "    self.label_indices = [annotation[0][5][0][0] for annotation in self.annotations]\n",
    "    #Extracting the car names as strings\n",
    "    self.car_names = [x[0] for x in self.mat['class_names'][0]]\n",
    "    #Create a list with car names instead of label indices for each sample\n",
    "    self.translated_car_names = [self.car_names[x-1] for x in self.label_indices]\n",
    "  def brand_types(self,base_dict, x):\n",
    "    y = list(base_dict.keys())[-1]\n",
    "    for k,v in base_dict.items():\n",
    "      if v in x: y=k\n",
    "    return y\n",
    "  def parsing(self):\n",
    "    result = []\n",
    "    for arg in self.args:\n",
    "      temp_list = [self.brand_types(arg,x) for x in self.translated_car_names]\n",
    "      result.append(temp_list)\n",
    "    if self.year != None:\n",
    "      years_list = [0 if int(x.split(\" \")[-1]) <= self.year else 1 for x in self.translated_car_names]\n",
    "      result.append(years_list)\n",
    "    brands = [x.split(\" \")[0] for x in self.translated_car_names]\n",
    "    return result, self.file_names, self.translated_car_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpuL_Csm1LBf"
   },
   "source": [
    "As described in the docstrings of the class, we can pass dictionaries that contain the characteristics for our class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ap4CiJwN-vZS",
    "outputId": "933b59f8-bf41-4c3e-c4f1-d0a18cb6c2a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_dict = {0: 'Audi', 1: 'BMW', 2: 'Chevrolet', 3: 'Dodge', 4: 'Ford', 5: 'Other'}\n",
    "vehicle_types_dict = {0: 'Convertible', 1: 'Coupe', 2: 'SUV', 3: 'Van', 4: 'Other'}\n",
    "\n",
    "results, file_names, translated_car_names = MetaParsing(\"content/car_metadata.mat\",brand_dict,vehicle_types_dict,year=2009).parsing()\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "088GSt8E1tkC"
   },
   "source": [
    "As expected, we get a list containing three lists of numeric features for our three classes (brand, type, year). These are our training labels. We can use the dictionaries to reassign them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbSq3dg1wOuS",
    "outputId": "1c934dfc-6050-4cbe-a3ae-139c25bd721a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audi: 1169\n",
      "BMW: 1055\n",
      "Chevrolet: 1799\n",
      "Dodge: 1253\n",
      "Ford: 1035\n",
      "Other: 9874\n",
      "Convertible: 1907\n",
      "Coupe: 2143\n",
      "SUV: 2855\n",
      "Van: 832\n",
      "Other: 8448\n"
     ]
    }
   ],
   "source": [
    "def count_classes(base_dict, base_list):\n",
    "  for i in range(len(list(base_dict.keys()))):\n",
    "    print(\"{}: {}\".format(base_dict[i], str(base_list.count(i))))\n",
    "\n",
    "count_classes(brand_dict,results[0])\n",
    "count_classes(vehicle_types_dict,results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojrqmu182lMo"
   },
   "source": [
    "At first glance, we have enough cases for each class. We do have skewed distributions, but we could mitigate that with weighting (https://discuss.pytorch.org/t/passing-the-weights-to-crossentropyloss-correctly/14731/10). We leave the classes as they are and create a dictionary for our custom dataset. We assign the corresponding tainings labels to each filename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "Bt-A2psYEgNQ"
   },
   "outputs": [],
   "source": [
    "translation_dict = dict(zip(file_names,list(zip(results[0],results[1],results[2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq4Nt7yr4aU-"
   },
   "source": [
    "Next we will create our custom dataset. For a deeper introduction you can have a look at this article of mine. Basically, there is nothing special yet. The only difference is that we load three tainingslabels for each sample instead of one, and pass all three into our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "ypYyTGY_JWf2"
   },
   "outputs": [],
   "source": [
    "class CarDataset(Dataset):\n",
    "  \n",
    "  def __init__(self,car_path,transform,translation_dict):\n",
    "    self.path = car_path\n",
    "    self.folder = [x for x in listdir(car_path)]\n",
    "    self.transform = transform\n",
    "    self.translation_dict = translation_dict\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.folder)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    img_loc = os.path.join(self.path, self.folder[idx])\n",
    "    image = Image.open(img_loc).convert('RGB')\n",
    "    single_img = self.transform(image)\n",
    "\n",
    "    label1 = translation_dict[self.folder[idx]][0]\n",
    "    label2 = translation_dict[self.folder[idx]][1]\n",
    "    label3 = translation_dict[self.folder[idx]][2]\n",
    "\n",
    "    sample = {'image':single_img, 'labels': {'label_brand':label1, 'label_vehicle_type':label2, 'label_epoch':label3}, 'path':img_loc}\n",
    "    return sample   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    paths = []\n",
    "    train_or_test = []\n",
    "    brand = []\n",
    "    vehicle_type = []\n",
    "    epoch = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        paths.extend(batch[\"path\"])\n",
    "        train_or_test.extend(list(np.zeros(len(batch[\"path\"]), dtype=int)))\n",
    "\n",
    "        brand.extend(batch[\"labels\"][\"brand\"])\n",
    "        vehicle_type.extend(batch[\"labels\"][\"vehicle_type\"])\n",
    "        epoch.extend(batch[\"labels\"][\"epoch\"])\n",
    "\n",
    "    for batch in test_loader:\n",
    "        paths.extend(batch[\"path\"])\n",
    "        train_or_test.extend(list(np.ones(len(batch[\"path\"]), dtype=int)))\n",
    "\n",
    "        brand.extend(batch[\"labels\"][\"brand\"])\n",
    "        vehicle_type.extend(batch[\"labels\"][\"vehicle_type\"])\n",
    "        epoch.extend(batch[\"labels\"][\"epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EF9w7_9UJ5IA",
    "outputId": "fc6e2636-1b8d-4acd-d7b0-817afdaee83a"
   },
   "outputs": [],
   "source": [
    "#Pre-processing transformations\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "#Getting the data\n",
    "cardata = CarDataset(\"stanford_cars/images\", transform=data_transforms,translation_dict=translation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, csv, root_dir, col_img_pth = \"filename\", cols_features = [], subset = \"train\",\n",
    "                 train_val_test = [.7,.1,.2], random_seed = 42, transform = None, shuffle = True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        if type(csv) == type(str):\n",
    "            self.csv = pd.read_csv(csv)\n",
    "        else:\n",
    "            self.csv = csv\n",
    "        self.root_dir = root_dir\n",
    "        self.imgs = [os.path.join(root_dir, img) for img in list(self.csv[col_img_pth])]\n",
    "        self.features = [list(self.csv[c]>.5) for c in cols_features]\n",
    "        if shuffle:\n",
    "            print(\"len should be equal\")\n",
    "            print(len(self.imgs))\n",
    "            np.random.seed(random_seed)\n",
    "            np.random.shuffle(self.imgs)\n",
    "        train_val_test = [int(len(self.imgs) * train_val_test[0]),\n",
    "                          int(len(self.imgs) * train_val_test[1]),\n",
    "                          int(len(self.imgs) * train_val_test[2])]\n",
    "        if subset == \"train\":\n",
    "            self.imgs = self.imgs[:train_val_test[0]]\n",
    "        elif subset == \"val\":\n",
    "            self.imgs = self.imgs[train_val_test[0]:train_val_test[0]+train_val_test[1]]\n",
    "        elif subset == \"test\":\n",
    "            self.imgs = self.imgs[train_val_test[0]+train_val_test[1]:]\n",
    "        elif subset == \"all\":\n",
    "            self.imgs = self.imgs[:]\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        self.features = np.array(self.features, dtype = bool).T\n",
    "        if shuffle:\n",
    "            print(len(self.features))\n",
    "            np.random.seed(random_seed)\n",
    "            np.random.shuffle(self.features)\n",
    "        print(self.features.shape)\n",
    "        if subset == \"train\":\n",
    "            self.features = self.features[:train_val_test[0]]\n",
    "        elif subset == \"val\":\n",
    "            self.features = self.features[train_val_test[0]:train_val_test[0]+train_val_test[1]]\n",
    "        elif subset == \"test\":\n",
    "            self.features = self.features[train_val_test[0]+train_val_test[1]:]\n",
    "        elif subset == \"all\":\n",
    "            self.features = self.features[:50]\n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "        self.transform = transform\n",
    "        self.cols_features =cols_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.imgs[idx]).convert('RGB')#torch.tensor(np.array(), dtype=np.float).permute(2,0,1).type(torch.float)\n",
    "        if type(self.transform) != type(None):\n",
    "            img = self.transform(img)\n",
    "        #labels = torch.tensor(self.features[idx], dtype=torch.float)\n",
    "        return {\"image\":img, \"labels\": {k:self.csv[k][idx] for k in self.cols_features}}\n",
    "    \n",
    "csv = pd.read_csv(\"stanford_cars/labels.csv\")\n",
    "root_dir = \"stanford_cars\"\n",
    "train_set = MultiLabelDataset(csv, root_dir, col_img_pth = \"path\", cols_features = [\"brand\", \"vehicle_type\", \"epoch\"], subset = \"train\",\n",
    "                              transform = data_transforms)\n",
    "val_set = MultiLabelDataset(csv, root_dir, col_img_pth = \"path\", cols_features = [\"brand\", \"vehicle_type\", \"epoch\"], subset = \"test\",\n",
    "                              transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data in training and testing\n",
    "\n",
    "if False:\n",
    "    train_len = int(cardata.__len__()*0.8)\n",
    "    test_len = int(cardata.__len__()*0.2)\n",
    "    train_set, val_set = torch.utils.data.random_split(cardata, [train_len, test_len])\n",
    "\n",
    "#Create the dataloader for each dataset\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, \n",
    "                                num_workers=4, drop_last=True)\n",
    "test_loader = DataLoader(val_set, batch_size=16, shuffle=False, \n",
    "                               num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv.drop(\"Unnamed: 0\", axis=1).to_csv(\"stanford_cars/labels.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    brand = [b.item() for b in brand]\n",
    "    vehicle_type = [t.item() for t in vehicle_type]\n",
    "    epoch = [e.item() for e in epoch]\n",
    "    df = pd.DataFrame()\n",
    "    df[\"path\"] = list(paths)\n",
    "    df[\"train_or_test\"] = list(train_or_test)\n",
    "    df[\"brand\"] = list(brand)\n",
    "    df[\"vehicle_type\"] = list(vehicle_type)\n",
    "    df[\"epoch\"] = list(epoch)\n",
    "    df.to_csv(\"labels1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GnLr_aC492i"
   },
   "source": [
    "We can load a sample with the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgdmcQJlJ8DI",
    "outputId": "c1ccceaf-6a91-4985-80cf-808b9ed851ed"
   },
   "outputs": [],
   "source": [
    "sample = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwBpN8lfesKX",
    "outputId": "cd29fc6a-ad30-4588-9463-d6672d9ceb4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['labels'][\"brand\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaGfopfzw7VP",
    "outputId": "69364d85-7b2b-4572-fbbb-91e08aed02b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in our sample batch: dict_keys(['image', 'labels'])\n",
      "Size for the images in our sample batch: torch.Size([16, 3, 224, 224])\n",
      "Size for the target in our sample batch: torch.Size([16])\n",
      "Targets for each batch in our sample: tensor([3, 5, 0, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys in our sample batch: {}\".format(sample.keys()))\n",
    "print(\"Size for the images in our sample batch: {}\".format(sample['image'].shape))\n",
    "print(\"Size for the target in our sample batch: {}\".format(sample['labels']['brand'].shape))\n",
    "print(\"Targets for each batch in our sample: {}\".format(sample['labels']['brand']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWSThqTC5E3o"
   },
   "source": [
    "Our custom dataset and the dataloader work as intended. We get one dictionary per batch with the images and 3 target labels. With this we have the prerequisites for our multilabel classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4qq9WjC1UTl",
    "outputId": "edd76abf-7dd7-4421-c537-aaaaf1f82590"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerstenberger/anaconda3/envs/optical_flow/lib/python3.9/site-packages/torchvision/models/_utils.py:207: UserWarning: The parameter 'pretrained' is deprecated, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gerstenberger/anaconda3/envs/optical_flow/lib/python3.9/site-packages/torchvision/models/_utils.py:220: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (2): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet34(pretrained=True)\n",
    "list(resnet.children())[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5DrLb_i1uRW"
   },
   "source": [
    "First, we load a pre-trained ResNet34 and display the last 3 children elements. First comes a sequential block, then a pooling operation and finally a linear layer. This gets 512 features as input and gives 1000 as output. We want to remove this last layer and replace it with new layers. We already know that we have 512 in-features each and need a) 6 out-featues for the brands, b) 5 out-features for the vehicle types and c) 2 out-features for the epochs. We can remove the last layer by putting all children elements into a list and removing the last element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "Sh2aQE4d1WPY"
   },
   "outputs": [],
   "source": [
    "model_wo_fc = nn.Sequential(*(list(resnet.children())[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu9twc6F3ZnJ"
   },
   "source": [
    "We can process an output with our resnet without a classifier head and look at the respective tensor shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oriyR7072v-M",
    "outputId": "97b5e389-7f54-49b4-cf3e-8b6c8b4ac7f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 1, 1])\n",
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "output_sample = model_wo_fc(sample['image'])\n",
    "print(output_sample.shape)\n",
    "print(torch.flatten(output_sample, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0t78tx13hTz"
   },
   "source": [
    "As a result we get a tensor with the format [16,512,1,1]. We have 16 samples in our batch and 512 features per image. The 3rd and 4th dimension has size 1 and can be smoothed by torch.flatten. We can now pass this output to our new classifier layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPUm8VV63gFW",
    "outputId": "9e5fc583-b617-4dac-dbcb-60aadbc3a6a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sample_flatten = torch.flatten(output_sample, 1)\n",
    "brand = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=512, out_features=6)\n",
    "        )\n",
    "brand(output_sample_flatten).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuvlQSP34owK"
   },
   "source": [
    "This is exactly what we wanted to have. We get 6 logits per sample in our batch. We can now process these as usual using a loss function in our training loop. Now we add the other two classifier layers and put everything together in a custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "Ah6f3XWOu7to"
   },
   "outputs": [],
   "source": [
    "class MultilabelClassifier(nn.Module):\n",
    "    def __init__(self, feature_categories = [], classes_of_categories = []):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet34(pretrained=True)\n",
    "        self.model_wo_fc = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        \n",
    "        self.classification_heads = []\n",
    "        self.feature_categories = feature_categories\n",
    "        \n",
    "        for n in classes_of_categories:\n",
    "            head = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(in_features=512, out_features=n)).to(device)\n",
    "            self.classification_heads.append(head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_wo_fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return {name:head(x) for name, head in zip(self.feature_categories, self.classification_heads)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "D8_LU-Pkw9ZK"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultilabelClassifier(feature_categories = [\"brand\", \"vehicle_type\", \"epoch\"], classes_of_categories =[6,5,2]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRPCHXUL1vRx"
   },
   "source": [
    "We create a flexible training routine that takes into account all outputs of our model. Therefore, it does not matter whether we have 2, 3 or, for example, 5 classifier heads. We simply use the conventional loss function for multiclassification tasks. We calculate the CrossEntropyLoss for each Head and sum the Losses. This way we can optimize the weights with a single optimizer step for all three heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d24AlzVmZPWG",
    "outputId": "1e4bcf48-5453-408c-da05-670b7649c023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [707/707], Loss: 3.4085\n",
      "Epoch [2/10], Step [707/707], Loss: 3.3458\n",
      "Epoch [3/10], Step [707/707], Loss: 3.3106\n",
      "Epoch [4/10], Step [707/707], Loss: 3.2639\n",
      "Epoch [5/10], Step [707/707], Loss: 3.1461\n",
      "Epoch [6/10], Step [707/707], Loss: 2.9003\n",
      "Epoch [7/10], Step [707/707], Loss: 2.6087\n",
      "Epoch [8/10], Step [707/707], Loss: 2.3461\n",
      "Epoch [9/10], Step [707/707], Loss: 2.1303\n",
      "Epoch [10/10], Step [707/707], Loss: 1.9545\n"
     ]
    }
   ],
   "source": [
    "def criterion(loss_func,outputs,pictures):\n",
    "    losses = 0\n",
    "    for i, key in enumerate(outputs):\n",
    "        losses += loss_func(outputs[key], pictures['labels'][f'{key}'].to(device))\n",
    "    return losses\n",
    "\n",
    "def training(model,device,lr_rate,epochs,train_loader):\n",
    "    num_epochs = epochs\n",
    "    losses = []\n",
    "    checkpoint_losses = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "    n_total_steps = len(train_loader)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, pictures in enumerate(train_loader):\n",
    "            images = pictures['image'].to(device)\n",
    "            pictures = pictures\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(loss_func,outputs, pictures)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % (int(n_total_steps/1)) == 0:\n",
    "                checkpoint_loss = torch.tensor(losses).mean().item()\n",
    "                checkpoint_losses.append(checkpoint_loss)\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {checkpoint_loss:.4f}')\n",
    "    return checkpoint_losses\n",
    "\n",
    "checkpoint_losses = training(model,device,0.0001,10,train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CE_MLutR2mWz"
   },
   "source": [
    "\n",
    "We also write the validation routine so that we can pass a flexible number of categories to be classified. We calculate both the total performance per class and the performance per category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UchfEVo-adQF",
    "outputId": "a36e4251-67f5-4ac2-98bf-dff77f3e10bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "\u001b[1m\u001b[91mOverall class performance: 44.1 %\u001b[0m\n",
      "Accuracy of Audi: 6.4 %\n",
      "Accuracy of BMW: 4.5 %\n",
      "Accuracy of Chevrolet: 6.5 %\n",
      "Accuracy of Dodge: 6.9 %\n",
      "Accuracy of Ford: 4.2 %\n",
      "Accuracy of Other: 67.3 %\n",
      "-------------------------------------------------\n",
      "\u001b[1m\u001b[91mOverall class performance: 34.4 %\u001b[0m\n",
      "Accuracy of Convertible: 11.0 %\n",
      "Accuracy of Coupe: 15.0 %\n",
      "Accuracy of SUV: 17.5 %\n",
      "Accuracy of Van: 2.9 %\n",
      "Accuracy of Other: 54.4 %\n",
      "-------------------------------------------------\n",
      "\u001b[1m\u001b[91mOverall class performance: 57.4 %\u001b[0m\n",
      "Accuracy of 2009 and earlier: 27.0 %\n",
      "Accuracy of 2010 and later: 71.9 %\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def validation(model, dataloader, *args):\n",
    "\n",
    "  all_predictions = torch.tensor([]).to(device)\n",
    "  all_true_labels = torch.tensor([]).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    n_correct = []\n",
    "    n_class_correct = []\n",
    "    n_class_samples = []\n",
    "    n_samples = 0\n",
    "\n",
    "    for arg in args:\n",
    "      n_correct.append(len(arg))\n",
    "      n_class_correct.append([0 for i in range(len(arg))])\n",
    "      n_class_samples.append([0 for i in range(len(arg))])\n",
    "\n",
    "    for pictures in dataloader:\n",
    "      images = pictures['image'].to(device)\n",
    "      outputs = model(images)\n",
    "      labels = [pictures['labels'][picture].to(device) for picture in pictures['labels']]\n",
    "\n",
    "      for i,out in enumerate(outputs):\n",
    "        _, predicted = torch.max(outputs[out],1)\n",
    "        n_correct[i] += (predicted == labels[i]).sum().item()\n",
    "\n",
    "        if i == 0:\n",
    "          n_samples += labels[i].size(0)\n",
    "\n",
    "        for k in range(16):\n",
    "          label = labels[i][k]\n",
    "          pred = predicted[k]\n",
    "          if (label == pred):\n",
    "              n_class_correct[i][label] += 1\n",
    "          n_class_samples[i][label] += 1\n",
    "          \n",
    "  return n_correct,n_samples,n_class_correct,n_class_samples\n",
    "\n",
    "def class_acc(n_correct,n_samples,n_class_correct,n_class_samples,class_list):\n",
    "    for i in range(len(class_list)):\n",
    "      print(\"-------------------------------------------------\")\n",
    "      acc = 100.0 * n_correct[i] / n_samples\n",
    "      print(color.BOLD + color.RED + f'Overall class performance: {round(acc,1)} %' + color.END)\n",
    "      for k in range(len(class_list[i])):\n",
    "          acc = 100.0 * n_class_correct[i][k] / n_class_samples[i][k]\n",
    "          print(f'Accuracy of {class_list[i][k]}: {round(acc,1)} %')\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "classes_brand = list(brand_dict.values())\n",
    "classes_vehicle_type = list(vehicle_types_dict.values())\n",
    "classes_epoch = ['2009 and earlier','2010 and later']\n",
    "class_list = [classes_brand,classes_vehicle_type,classes_epoch]\n",
    "\n",
    "n_correct,n_samples,n_class_correct,n_class_samples = validation(model,test_loader,classes_brand,classes_vehicle_type,classes_epoch)\n",
    "\n",
    "class_acc(n_correct,n_samples,n_class_correct,n_class_samples,class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaP0Dh0A27w0"
   },
   "source": [
    "With about 90% accuracy per class, we were able to make good predictions. We saw that we can classify multiple classes with one model without needing multiple models or runs. In our example, we used PyTorch and saw that we can quickly create a custom training routine with a custom dataset and a custom model. Furthermore, we took advantage of transfer learning to get good results quickly despite the complexity of the task. In the real world, there are many such application areas. Imagine you run a used car platform and want to extract suggestions for individual vehicle features directly from the images. We are not that far away from that in our example. There is another form of multilabel classification. Think of image tags in social networks, for example. Here, one has also given certain class, but not every image is forcibly assigned to every class. We will address this issue in the next chapter. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02_article_pytorch_multilabel_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "optical_flow",
   "language": "python",
   "name": "optical_flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
